# Implementation Overview Bevy Terrain

This document serves as a general overview of the implementation of the `bevy_terrain` plugin.

Currently, this crate provides two fundamental capabilities. 
For one, the UDLOD algorithm approximates the terrain geometry, and for another, the Chunked Clipmap stores the terrain data in a convenient and randomly accessible data structure. 

Both are described in detail in my [bachelor thesis]([https://github.com/kurtkuehnert/terrain_renderer/blob/main/Thesis.pdf](https://github.com/kurtkuehnert/terrain_renderer/blob/main/Thesis.pdf)). 
To understand the implementation of this crate and the reasons behind some design decisions, I recommend that you read at least the entire chapter 4. 
If you are unfamiliar with terrain rendering in general, taking a look at chapter 2 will prove beneficial as well.  

In the following, I will now explain how both of these systems are currently implemented, what limitations they possess, and how they should work in the future.
Furthermore, I have listed a couple of todos outlining the essence of these issues. 
If any of them sound interesting to you, and you would like to work on them, please get in touch with me, so we can discuss solutions. 
These are certainly not all problems of the current implementation, so if you notice anything else, please let me know, so I can add it here.

## Terrain Geometry

### Ideal

Ideally, we would like to represent the terrain geometry without any error according to our source data. Unfortunately, rendering each data point as a vertex is not scalable, nor efficient. 

### Reality

That is why we need a sophisticated level of detail (LOD) algorithm that minimizes the error introduced by its approximation of the geometry.

### Solution

One such solution is the Uniform Distance-Dependent Level of Detail (UDLOD) algorithm that I have developed as part of my thesis (for a detailed explanation read section 4.4). 
It divides the terrain into numerous small tiles in parallel on the GPU. They are then rendered using a single indirect draw call and morphed together (in the vertex shader) to form a continuous surface with an approximately uniform tessellation in screen space.

### Issues

For any LOD algorithm, an appropriate crack-avoiding and morphing strategy are important to eliminate and reduce visual discrepancies as much as possible. 
UDLOD uses a slightly modified version of the CDLOD morphing scheme.

The UDLOD algorithm can be used to tessellate procedural ground details like rocks or cobblestones as well. 
Therefore, simply increase the quadtree depth using the `additional_refinement` parameter.

Even though the tessellation produced by UDLOD is somewhat uniform with respect to the distance, it does not take factors like the terrain's roughness and the viewing angle into account.
Generally, the current UDLOD algorithm tiers to cover the worst-case terrain roughness like many other algorithms (GeoMipmap, GeoClipmap, PGM, CDLOD, FarCry5). 
I believe that we can still develop more efficient LOD algorithms that scale favorably for large-scale terrains in the future.

The culling is currently pretty bare-bones. 
We could probably implement most of the techniques researched by the Far Cry 5 terrain renderer as well. 

Currently, the prepass is pretty inefficient, because the shader occupancy is very low (the prepass is still plenty fast, but could be improved). 
I think that this could be resolved by using the atomic operations more cleverly and reducing the shader dispatches in general. 
In the past, I have tried doing all the work in a single pass. 
Unfortunately, that did not work, but maybe someone can figure out a better solution.

The frustum culling uses a 2D min-max height data attachment to approximate the bounding volumes of each tile correctly. 
This is currently stored with the same resolution as the source height data, but only a fraction of this resolution is actually required. 

### Todo

- [x]  come up with a smooth morphing strategy that solves the geometry crack problem as well
- [x]  implement bounding box frustum culling
- [x]  further refine the geometry for procedural details (rocks, cobblestone)
- [ ]  explore different LOD algorithms (maybe apply the clipmap idea to CBTs?)
- [ ]  try incorporating a screen space error metric, local terrain roughness, or the viewing angle
- [ ]  implement more advanced culling solutions (occlusion, backface)
- [ ]  try reducing the compute shader dispatches in the prepass phase
- [ ]  store min-max height data, required by frustum culling, at a way lower resolution
- [ ]  experiment with hardware tessellation or mesh shaders

## Terrain Data

### Ideal

Ideally, we would like to store any desired information at any desired resolution across the terrainâ€™s surface. 
For example, a terrain could require a heightmap with a resolution of 0.5m, an albedo map with a resolution of 0.2m, and a vegetation map (for placing trees and bushes) with a resolution of 1m. 
Each of these three different kinds of terrain data are called terrain attachments. 
This terrain data should be available in any system and shader of our application. Additionally, we would like to access the data at any position and sample a value with distant-dependent accuracy. 
Finally, some use cases require the ability to sample some attachments like the albedo or splat data trilinearly and anisotropically to mitigate aliasing artifacts.

### Reality

Because we are using height-map-based terrain these attachments should be stored as large two-dimensional textures. 
However, due to the size of most landscapes, using a single texture would quickly use up all of our video memory. 
That is why we need to partition and adjust the loaded data according to our view. 
Additionally, it is important that we can share this terrain data efficiently between multiple views for use cases like split-screen or shadow rendering.

### Solution

To solve this, I have developed the chunked clipmap data structure (if you are unfamiliar with the concept, I encourage you to read section 4.5 of my thesis). 
It divides the entire terrain data into one large quadtree, covering the entire terrain. 
This requires that all terrain data has to be preprocessed into small square textures: the nodes of the quadtree. 
Each node possesses one texture per attachment. To allow for different resolutions of the attachments (e.g. the height data should be twice as accurate as our splat map), the size of these textures has to be different as well.
Following the same example, this would mean that the height textures would have a size of 100x100 and the splat textures a size of 50x50 pixels.

### Issues

Because of our compound representation of the terrain data, consisting of many small textures, some problems arise during texture filtering. 
The biggest issue is that adjacent tiles do not line up perfectly due to missing texture information at the border. 
This causes noticeable texture seams between adjacent nodes. 
To remedy this issue we have to duplicate the border data between adjacent nodes. 
This complicates our preprocessing but results in a completely seamless terrain data representation.

For trilinear filtering, we additionally require mipmap information. 
Currently, bevy does not support mipmap generation. 
That is why I have implemented a simple mipmap creation function, which is executed after the node textures have been loaded. Unfortunately, my simple approach only works on textures with a side length equal to a power of two (e.g. 256x256, 512x512). 
This needlessly limits the resolutions of our terrain data. 
In the future, I would like to generate the mipmaps for any texture size.

As mentioned above the terrain data has to be loaded depending on our current view position. 
Currently, I load all nodes inside the `load_distance` around the viewer. 
There is no prioritization or load balancing. I would like to explore different loading strategies (e.g. distance only, view frustum based, etc.) to enable use cases like streaming data from a web server. 
For that, the strategy would have to minimize the loading requests while maximizing the visual quality. 
When streaming from disk this wasn't a problem yet.

Additionally, the plugin panics if the node atlas is out of indices (i.e. the maximum amount of nodes is loaded). 
This is unacceptable in production use. 
Here we would have to come up with a strategy of prioritizing which nodes to keep and which ones to discard in order to accommodate more important ones.

The node loading code itself is currently pretty inefficient. 
Due to the nature of the bevy image abstraction, all textures are duplicated multiple times. 
Hopefully in the near future, once the asset processing has been reworked, it will be easier to express loading parts of an array texture directly.

To divide the terrain into the numerous node textures I use a 3-step preprocessing algorithm. 
This is implemented pretty inefficiently. 
If you are interested in optimizing data transformation code, this should be the task for you :D.

To save space the terrain data is compressed using common image formats, when it is stored on the hard-drive. 
To unfortunately the encoding of PNGs is quite slow. 
That is why I came up with the [DTM image format](https://github.com/kurtkuehnert/dtm). 
It uses a sequential compression technique similar to the QOI format.
DTM works quite well for the shallow terrain I used for testing, but is not ideal for the steep and hilly terrains used in most games.
There are probably significant gains to be had in this area.

Another huge challenge regarding the terrain data is its modification in real-time. 
Workflows like sculpting, texturing, etc. do require the ability to update the terrain data in a visual manner. 
This topic is vast and will require extensive investigation before we can settle on a final design. 
If you have experience/ideas please let me know.

### Todo

- [x]  duplicate border information to eliminate texture seams
- [x]  generate mipmaps to enable trilinear filtering
- [ ]  Incorporate better mipmap generation for any texture size.
- [ ]  different loading strategies
- [ ]  handle node atlas out of indices
- [ ]  improve loading to node atlas (i.e. loading layers of an array texture), remove excessive duplication/copying
- [ ]  improve the preprocessing with caching, GPU acceleration, etc.
- [ ]  explore the usage of more efficient image formats
- [ ]  investigate real-time modification
