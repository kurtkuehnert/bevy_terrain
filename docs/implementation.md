# Implementation Overview Bevy Terrain

This document serves as a general overview of the implementation of the `bevy_terrain` plugin.

Currently, this crate provides two fundamental capabilities.
For one, the UDLOD algorithm approximates the terrain geometry, and for another, the Chunked Clipmap stores the terrain
data in a convenient and randomly accessible data structure.

Both are described in detail in
my [bachelor thesis]([https://github.com/kurtkuehnert/terrain_renderer/blob/main/Thesis.pdf](https://github.com/kurtkuehnert/terrain_renderer/blob/main/Thesis.pdf)).
To understand the implementation of this crate and the reasons behind some design decisions, I recommend that you read
at least the entire chapter 4.
If you are unfamiliar with terrain rendering in general, taking a look at chapter 2 will prove beneficial as well.

In the following, I will now explain how both of these systems are currently implemented, what limitations they possess,
and how they should work in the future.
Furthermore, I have listed a couple of todos outlining the essence of these issues.
If any of them sound interesting to you, and you would like to work on them, please get in touch with me, so we can
discuss solutions.
These are certainly not all problems of the current implementation, so if you notice anything else, please let me know,
so I can add it here.

## Terrain Geometry

### Ideal

Ideally, we would like to represent the terrain geometry without any error according to our source data. Unfortunately,
rendering each data point as a vertex is not scalable, nor efficient.

### Reality

That is why we need a sophisticated level of detail (LOD) algorithm that minimizes the error introduced by its
approximation of the geometry.

### Solution

One such solution is the Uniform Distance-Dependent Level of Detail (UDLOD) algorithm that I have developed as part of
my thesis (for a detailed explanation read section 4.4).
It divides the terrain into numerous small tiles in parallel on the GPU. They are then rendered using a single indirect
draw call and morphed together (in the vertex shader) to form a continuous surface with an approximately uniform
tessellation in screen space.

### Issues

For any LOD algorithm, an appropriate crack-avoiding and morphing strategy are important to eliminate and reduce visual
discrepancies as much as possible.
UDLOD uses a slightly modified version of the CDLOD morphing scheme.

The UDLOD algorithm can be used to tessellate procedural ground details like rocks or cobblestones as well.
Therefore, simply increase the tile_tree depth using the `additional_refinement` parameter.

Even though the tessellation produced by UDLOD is somewhat uniform with respect to the distance, it does not take
factors like the terrain's roughness and the viewing angle into account.
Generally, the current UDLOD algorithm tiers to cover the worst-case terrain roughness like many other algorithms (
GeoMipmap, GeoClipmap, PGM, CDLOD, FarCry5).
I believe that we can still develop more efficient LOD algorithms that scale favorably for large-scale terrains in the
future.

The culling is currently pretty bare-bones.
We could probably implement most of the techniques researched by the Far Cry 5 terrain renderer as well.

Currently, the prepass is pretty inefficient, because the shader occupancy is very low (the prepass is still plenty
fast, but could be improved).
I think that this could be resolved by using the atomic operations more cleverly and reducing the shader dispatches in
general.
In the past, I have tried doing all the work in a single pass.
Unfortunately, that did not work, but maybe someone can figure out a better solution.

The frustum culling uses a 2D min-max height data attachment to approximate the bounding volumes of each tile correctly.
This is currently stored with the same resolution as the source height data, but only a fraction of this resolution is
actually required.

### Todo

- [x]  come up with a smooth morphing strategy that solves the geometry crack problem as well
- [x]  implement bounding box frustum culling
- [x]  further refine the geometry for procedural details (rocks, cobblestone)
- [ ]  explore different LOD algorithms (maybe apply the clipmap idea to CBTs?)
- [ ]  try incorporating a screen space error metric, local terrain roughness, or the viewing angle
- [ ]  implement more advanced culling solutions (occlusion, backface)
- [ ]  try reducing the compute shader dispatches in the prepass phase
- [ ]  store min-max height data, required by frustum culling, at a way lower resolution
- [ ]  experiment with hardware tessellation or mesh shaders

## Terrain Data

### Ideal

Ideally, we would like to store any desired information at any desired resolution across the terrainâ€™s surface.
For example, a terrain could require a heightmap with a resolution of 0.5m, an albedo map with a resolution of 0.2m, and
a vegetation map (for placing trees and bushes) with a resolution of 1m.
Each of these three different kinds of terrain data are called terrain attachments.
This terrain data should be available in any system and shader of our application. Additionally, we would like to access
the data at any position and sample a value with distant-dependent accuracy.
Finally, some use cases require the ability to sample some attachments like the albedo or splat data trilinearly and
anisotropically to mitigate aliasing artifacts.

### Reality

Because we are using height-map-based terrain these attachments should be stored as large two-dimensional textures.
However, due to the size of most landscapes, using a single texture would quickly use up all of our video memory.
That is why we need to partition and adjust the loaded data according to our view.
Additionally, it is important that we can share this terrain data efficiently between multiple views for use cases like
split-screen or shadow rendering.

### Solution

To solve this, I have developed the chunked clipmap data structure (if you are unfamiliar with the concept, I encourage
you to read section 4.5 of my thesis).
It divides the entire terrain data into one large tile_tree, covering the entire terrain.
This requires that all terrain data has to be preprocessed into small square textures: the tiles of the tile_tree.
Each tile possesses one texture per attachment. To allow for different resolutions of the attachments (e.g. the
height data should be twice as accurate as our splat map), the size of these textures has to be different as well.
Following the same example, this would mean that the height textures would have a size of 100x100 and the splat textures
a size of 50x50 pixels.

### Issues

Because of our compound representation of the terrain data, consisting of many small textures, some problems arise
during texture filtering.
The biggest issue is that adjacent tiles do not line up perfectly due to missing texture information at the border.
This causes noticeable texture seams between adjacent tiles.
To remedy this issue we have to duplicate the border data between adjacent tiles.
This complicates our preprocessing but results in a completely seamless terrain data representation.

For trilinear filtering, we additionally require mipmap information.
Currently, bevy does not support mipmap generation.
That is why I have implemented a simple mipmap creation function, which is executed after the tile textures have
been loaded. Unfortunately, my simple approach only works on textures with a side length equal to a power of two (e.g.
256x256, 512x512).
This needlessly limits the resolutions of our terrain data.
In the future, I would like to generate the mipmaps for any texture size.

As mentioned above the terrain data has to be loaded depending on our current view position.
Currently, I load all tiles inside the `load_distance` around the viewer.
There is no prioritization or load balancing. I would like to explore different loading strategies (e.g. distance only,
view frustum based, etc.) to enable use cases like streaming data from a web server.
For that, the strategy would have to minimize the loading requests while maximizing the visual quality.
When streaming from disk this wasn't a problem yet.

Additionally, the plugin panics if the tile atlas is out of indices (i.e. the maximum amount of tiles is
loaded).
This is unacceptable in production use.
Here we would have to come up with a strategy of prioritizing which tiles to keep and which ones to discard in
order to accommodate more important ones.

The tile loading code itself is currently pretty inefficient.
Due to the nature of the bevy image abstraction, all textures are duplicated multiple times.
Hopefully in the near future, once the asset processing has been reworked, it will be easier to express loading parts of
an array texture directly.

To divide the terrain into the numerous tile textures I use a 3-step preprocessing algorithm.
This is implemented pretty inefficiently.
If you are interested in optimizing data transformation code, this should be the task for you :D.

To save space the terrain data is compressed using common image formats, when it is stored on the hard-drive.
To unfortunately the encoding of PNGs is quite slow.
That is why I came up with the [DTM image format](https://github.com/kurtkuehnert/dtm).
It uses a sequential compression technique similar to the QOI format.
DTM works quite well for the shallow terrain I used for testing, but is not ideal for the steep and hilly terrains used
in most games.
There are probably significant gains to be had in this area.

Another huge challenge regarding the terrain data is its modification in real-time.
Workflows like sculpting, texturing, etc. do require the ability to update the terrain data in a visual manner.
This topic is vast and will require extensive investigation before we can settle on a final design.
If you have experience/ideas please let me know.

### Todo

- [x]  duplicate border information to eliminate texture seams
- [x]  generate mipmaps to enable trilinear filtering
- [ ]  Incorporate better mipmap generation for any texture size.
- [ ]  different loading strategies
- [ ]  handle tile atlas out of indices
- [ ]  improve loading to tile atlas (i.e. loading layers of an array texture), remove excessive
  duplication/copying
- [ ]  improve the preprocessing with caching, GPU acceleration, etc.
- [ ]  explore the usage of more efficient image formats
- [ ]  investigate real-time modification
